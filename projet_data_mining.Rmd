---
title: "Projet en Data Mining"
author: "Sadoune Abdelkarim & Chetouani Yassine"
date: "19/12/2021"
output:
  html_document:
    toc: yes
    number_sections: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: inline
---
\newpage
# introduction
La méthode des K-Means (méthode des centres mobiles) est une technique de classification
automatique (clustering en anglais). Elle vise à produire un regroupement de manière à ce que les
individus du même groupe soient semblables, les individus dans des groupes différents soient
dissemblables.

# la base de données
Nous utilisons le fichier ***cars_dataset.txt***, un fichier texte avec séparateur tabulation. Il décrit
les caractéristiques de 392 véhicules. Les variables actives qui participeront au calcul sont : la
consommation (MPG, miles per galon, plus le chiffre est élevé, moins la voiture consomme) ; la
taille du moteur (DISPLACEMENT) ; la puissance (HORSEPOWER) ; le poids (WEIGHT) et
l’accélération (ACCELERATION, le temps mis pour atteindre une certaine vitesse, plus le chiffre est
faible plus la voiture est performante).
La variable illustrative « origine des véhicules » (ORIGIN : Japon, Europe, Etats Unis) servira à
renforcer l’interprétation des groupes.

# L'algorithme de K-Means 
<pre>
<strong>Entrée :</strong><br />
X (n obs., p variables), K #classes <br />
Initialiser K centres de classes G(k) <br /> 

<strong>REPETER :</strong><br />
- Allocation. Affecter chaque individu à la
classe dont le centre est le plus proche.<br />
- Représentation. Recalculer les centres de
classes à partir des individus rattachés<br />
<strong>JUSQU’À</strong> Convergence. <br />
<strong>Sortie :</strong> <br />
Une partition des individus
caractérisée par les K centres de classes Gk<br />
</pre>
# Application de K-Means sur la base de données cars sous R
## importation des données
```{r}
cars <- read.table("cars_dataset.txt", header=T, dec=".")
summary(cars)

```

## centrage et reduction
nous appliquons le centrage et la réduction sur les variables quantitatives
```{r}
centrage_reduction <- function(x) {
  return((x-mean(x))/sqrt(var(x)))
}

cars.cr <- apply(cars[,1:5], 2, centrage_reduction)
head(cars.cr)
```

## la détection d'un nombre adéquat de groupes
```{r}
nb.essais <- 5
inertie.expl <- rep(0, 10)
for(k in 2:10){
  cars_km <- kmeans(cars.cr, centers = k, nstart = nb.essais)
  inertie.intra[k] <- cars_km$tot.withinss
}

plot(2:10, inertie.expl[-1],type="b",xlab="nombre de groupes",ylab="inertie inta-classes")
abline(v = 4, col = "blue", lwd = 2, lty = 3) 
```

aprés le 4 il y'a pas une dégradation importantes, donc nous allons choisir 4 comme nombre de classes.

## appliquer le kmeans sur les variables centrées réduites avec 4 classes et 5 nombre d'essais avec différents individus de départ
```{r}
nb.classes <- 4
nb.essais <- 5
km <- kmeans(cars.cr, centers = nb.classes, nstart = nb.essais)
print(km)
```
le pourcentage d'inertie expliquée par cette partition est 76.8%

## récupération des groupes d'apparetenance
```{r}
groupe <- km$cluster
```

## le calcule des barycentres des classes dans l'espace des variables actives initiales
```{r}
centres <- NULL
for (k in 1:nb.classes){
  ligne <- colMeans(cars[groupe==k,1:5])
  centres <- rbind(centres,ligne)
}
numero <- seq(from=1,to=nb.classes)
rownames(centres) <- paste("classe_",numero,sep="")
print(centres)
```

## croiser les classes avec la variable qualitative en produisant un tableau de contingence
```{r}
print(table(cars$origin, groupe))
```

## graphique des variables 2 à 2 avec groupe d'appaternance
```{r}
pairs(cars[,1:5],pch=21,bg=c("red","blue","yellow","green")[groupe])
```

les variables sont pour la plupart fortement
corrélées, presque tous les couples de variables permettent de distinguer les groupes.

## ACP sur les données centrées réduites
nous allons utiliser l'ACP pour que nous puissions projeter les points dans le premier plan factoriel.

```{r}
pca <- princomp(cars.cr,scores=T)
print(pca)
```

## pour obtenir les valeurs propres et inertie expliquées par chaque composante principale
```{r}
pca.var <- pca$sdev^2
print(pca.var)
```

```{r}
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
print(pca.var.per)
barplot(pca.var.per, xlab = "composante principale", ylab = "pourcentage de l'inertie")
```

nous gardons les deux premiers axes qui portent la majorité d'inertie

## graphique dans le premier plan factoriel, avec mise en évidence des groupes
```{r}
plot(pca$scores[,1],pca$scores[,2],type="p",pch=20,col=c("red","blue","yellow","green")[groupe],
     xlab = paste("PC1 - ", pca.var.per[1], "%", sep = ""), ylab = paste("PC2 - ", pca.var.per[2], "%", sep = ""))
```


## exportation des données avec le cluster d'appartenance
```{r}
cars.output <- cbind(cars,groupe)
write.table(cars.output,file="cars_output.txt",sep="\t",dec=".",row.names=F)
head(cars.output)
```

# Conclusion
- Les techniques de partitionnement par réallocation présentent
l’avantage de la simplicité.
- Elles peuvent traiter de très grandes bases, mais sont lentes car
requièrent plusieurs passages sur la base de données.
- Elles produisent des classes convexes, centrées sur les barycentres
conditionnels (méthode des centres mobiles).
- La démarche peut être étendue aux cas des variables actives
qualitatives ou mixtes.
- La démarche peut être étendue à la classification de variables.
- Le choix du nombre de classes K reste un problème ouvert.



